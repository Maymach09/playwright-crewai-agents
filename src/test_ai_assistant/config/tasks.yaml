plan_test_task:
  description: >
    Create a comprehensive test plan by exploring the application.

    **INPUTS:**
    - user_input: {user_input} (the scenario or feature to test)
    - context: {context} (any additional context or requirements)

    **STEP-BY-STEP WORKFLOW:**
    
    Step 1: Setup the browser environment
    - Call `planner_setup_page` with these exact parameters:
      * project: "chromium"
      * seedFile: "tests/seed.spec.ts"
    - Wait for the page to load completely
    
    Step 2: Explore the application
    - Use `browser_snapshot` to see the current page structure
    - Identify all interactive elements (buttons, links, forms, inputs)
    - Note the exact text labels, roles, and element types
    
    Step 3: Navigate and discover features
    - Use `browser_click` to navigate through different sections
    - Use `browser_snapshot` after each navigation to document new pages
    - Map out the complete user flow for the scenario
    
    Step 4: Document findings
    - Create detailed test scenarios with:
      * Clear descriptive titles
      * Numbered steps (1, 2, 3...)
      * Specific element details (button text, field labels, exact wording)
      * Expected results after each step
    - Include both happy path and edge cases
    
    Step 5: Save the test plan
    - Use `fs_write_file` to save to test_plan/<timestamp>_test-plan.md
    - Use markdown format with clear sections and headings
    
    **OUTPUT FORMAT:**
    ```markdown
    # Application - Feature Test Plan
    
    ## Application Overview
    [Brief description of what you observed]
    
    ## Test Scenarios
    
    ### 1. Scenario Name
    
    **Seed:** tests/seed.spec.ts
    
    #### 1.1 Sub-scenario Name
    
    **Steps:**
    1. [Action with specific element details]
    2. [Action with specific element details]
    3. [Action with specific element details]
    
    **Expected Results:**
    - Step 1: [What should happen]
    - Step 2: [What should happen]
    - Step 3: [What should happen]
    ```
    
    **IMPORTANT:**
    - Base everything on actual browser_snapshot outputs
    - Do NOT make assumptions about elements
    - Include exact element text and labels
    - Number all steps clearly
    - Make scenarios detailed enough for automated test generation

  expected_output: >
    A complete test plan markdown file saved to test_plan/<timestamp>_test-plan.md
    containing detailed scenarios with numbered steps, specific element details,
    and clear expected results.


generate_test_task:
  description: >
    Generate executable Playwright test scripts from the test plan.

    **INPUTS:**
    - user_input: {user_input} (which scenarios to generate)
    - context: {context} (the complete test plan from planner)

    **CRITICAL: Read the context carefully! It contains the full test plan with all steps.**

    **STEP-BY-STEP WORKFLOW FOR EACH SCENARIO:**
    
    Step 1: Identify the scenario
    - Read user_input to know which scenario to generate
    - Find that scenario in the context (test plan)
    - Extract all steps and expected results
    
    Step 2: Setup the test environment
    - Call `generator_setup_page` with:
      * project: "chromium"
      * seedFile: "tests/seed.spec.ts"
      * plan: (brief description of what you're testing)
    - Wait for confirmation that page is ready
    
    Step 3: Execute each test step manually
    - For each step in the test plan:
      a. IMPORTANT: Use `browser_snapshot` FIRST to see current page state and get fresh element refs
      b. Find the correct element in the snapshot (ignore refs from test plan - they're outdated)
      c. Execute the action:
         - `browser_click` for clicking buttons/links (use ref from current snapshot)
         - `browser_type` for typing in single fields (use ref from current snapshot)
         - `browser_fill_form` for multiple form fields at once
      d. After each action, wait briefly if needed using `browser_wait_for`
    - Continue until all steps are executed
    
    Step 4: Capture the recorded actions
    - Call `generator_read_log` (no parameters needed)
    - This returns all the Playwright code that was executed
    - Review the code to ensure it matches the test plan
    
    Step 5: Write the test file (MANDATORY - DO NOT SKIP)
    - Call `generator_write_test` tool with these exact parameters:
      * fileName: "tests/descriptive-scenario-name.spec.ts" (MUST include 'tests/' prefix)
      * code: Complete TypeScript test code from generator_read_log
    - The tool will automatically save to the tests/ directory
    - Verify the file was created successfully
    
    **CODE STRUCTURE TEMPLATE:**
    ```typescript
    // seed: tests/seed.spec.ts
    // spec: test_plan/<timestamp>_test-plan.md
    
    import { test, expect } from '@playwright/test';
    
    test.use({ storageState: 'auth_state.json' });
    
    test.describe('Feature Name', () => {
      test('Scenario Name', async ({ page }) => {
        // 1. [Step description from test plan]
        await page.goto('url');
        await expect(page).toHaveURL('url');
        
        // 2. [Step description from test plan]
        await page.getByRole('button', { name: 'Text' }).click();
        await expect(page.getByText('Result')).toBeVisible();
        
        // 3. [Continue for each step...]
      });
    });
    ```
    
    **CODE REQUIREMENTS:**
    - Include ALL steps from the test plan
    - Add comments before each step (// 1. Description)
    - Use proper Playwright locators:
      * getByRole() - preferred
      * getByText() - for text content
      * getByLabel() - for form fields
      * getByPlaceholder() - for inputs
    - Add assertions after important actions:
      * expect(page).toHaveURL()
      * expect(page).toHaveTitle()
      * expect(element).toBeVisible()
      * expect(element).toHaveValue()
      * expect(element).toHaveText()
    - Use proper TypeScript syntax
    - One test file per scenario
    
    **IMPORTANT RULES:**
    - ALWAYS call generator_setup_page FIRST
    - ALWAYS take a NEW snapshot BEFORE each action (element refs change after each action!)
    - DO NOT use element refs from the test plan - they are from exploration time and are outdated
    - ALWAYS get fresh refs from the current browser_snapshot
    - ALWAYS call generator_read_log BEFORE writing test
    - ALWAYS call generator_write_test to save the file
    - DO NOT skip any steps from the test plan
    - DO NOT invent steps not in the test plan
    - Follow the test plan EXACTLY

  expected_output: >
    Valid Playwright test files saved to tests/ directory. Each file must:
    - Be valid TypeScript that compiles
    - Follow Playwright best practices
    - Include proper imports and test.use()
    - Have descriptive test.describe() and test() names
    - Include comments for each step
    - Have proper assertions
    - Be immediately executable


heal_test_task:
  description: >
    Debug and fix failing Playwright tests systematically.

    **INPUTS:**
    - user_input: {user_input} (which tests to heal or "all")
    - context: {context} (test file paths or location)

    **CRITICAL RULES TO PREVENT CONTEXT OVERFLOW:**
    - Fix ONE test at a time, then move to next
    - Maximum 2 fix attempts per test (if still fails, mark as fixme)
    - **NEVER use browser_snapshot or browser debugging tools**
    - **Work ONLY with error messages from test runs**
    - Playwright error messages contain all information needed to fix
    - Focus on error message patterns, not browser inspection
    - Keep fixes minimal and targeted
    
    **TOOL CALL FORMAT (IMPORTANT):**
    When calling `playwright_test_run_test`, ALWAYS use array format:
    ```python
    # ✅ CORRECT - locations is an array
    playwright_test_run_test(locations=["tests/"], projects=["chromium"])
    playwright_test_run_test(locations=["tests/test.spec.ts"], projects=["chromium"])
    
    # ❌ WRONG - locations is a string
    playwright_test_run_test(locations="tests/", projects=["chromium"])
    ```

    **STEP-BY-STEP WORKFLOW:**
    
    Step 1: Identify failing tests
    - Call `playwright_test_run_test` with:
      * locations: ["tests/"] (MUST be a list/array, not a string)
      * projects: ["chromium"]
    - Review the output to identify which tests failed
    - Note ONLY the error type and message (don't save full output)
    - Count failing tests and plan to fix them ONE BY ONE
    
    Step 2: For FIRST failing test - Analyze error from message
    - Extract the error message from Step 1 test run output
    - **DO NOT use browser tools - error message has everything you need**
    - Playwright errors show:
      * Exact error type (strict mode violation, timeout, assertion failed)
      * Elements found (with their locators and attributes)
      * Suggested fixes (often in the error itself)
    - Match error to COMMON FIXES patterns below
    - Identify the exact fix needed
    - Go directly to Step 4 (apply fix)
    
    Step 3: (SKIP THIS STEP - NOT NEEDED)
    - Playwright error messages are detailed enough to fix without browser debugging
    - Go directly to Step 4
    
    Step 4: Apply the fix
    - Use `fs_read_file` to read the test file
    - Apply the appropriate fix from COMMON FIXES patterns:
      * Strict mode violation → Use more specific locator
      * Multiple elements → Add exact: true or use role
      * URL mismatch → Fix regex pattern
      * Regex special chars → Escape them
    - Use `fs_write_file` to save the fixed test
    - Keep the fix minimal (only change the failing line)
    
    Step 5: Verify the fix (MAX 1 retry)
    - Call `playwright_test_run_test` with:
      * locations: ["tests/test.spec.ts"] (the specific test file as a list)
      * projects: ["chromium"]
    - If PASSES: Move to next failing test (go to Step 2)
    - If FAILS: Read new error message
      * If same error: Mark as test.fixme() with explanation
      * If different error: Try fix ONE more time (go to Step 4)
    - After 2 attempts total, mark as fixme and move on
    
    Step 6: Repeat for remaining tests
    - Return to Step 2 for next failing test
    - Continue until all tests are fixed or marked as fixme
    - Generate summary report at the end
    
    **COMMON FIXES:**
    ```typescript
    // ERROR MESSAGE EXAMPLE:
    // "Error: strict mode violation: getByLabel('Account Name') resolved to 3 elements:
    //     1) <th scope="col" title="Account Name">
    //     2) <input type="range" aria-label="Account Name Column Width">
    //     3) <input name="Name" type="text"> aka getByRole('textbox', { name: 'Account Name' })"
    
    // 1. Strict mode violation - multiple elements found
    // FIX: Use the specific locator suggested in error (option 3)
    // Before: page.getByLabel('Account Name')
    // After:  page.getByRole('textbox', { name: 'Account Name' })
    
    // ERROR MESSAGE EXAMPLE:
    // "Error: strict mode violation: getByRole('button', { name: 'Save' }) resolved to 2 elements:
    //     1) 'Save & New'
    //     2) 'Save'"
    
    // 2. Multiple buttons with similar names
    // FIX: Add exact: true
    // Before: page.getByRole('button', { name: 'Save' })
    // After:  page.getByRole('button', { name: 'Save', exact: true })
    
    // 3. Multiple elements with same text
    // Before: page.getByText('Test Account 2')
    // After:  page.getByRole('heading', { name: /Test Account 2/ })
    
    // 4. URL pattern too strict
    // Before: expect(page).toHaveURL(/.*Accounts/)
    // After:  expect(page).toHaveURL(/.*Account.*/)
    
    // 5. Regex special characters in title
    // Before: expect(page).toHaveTitle(/Recently Viewed | Accounts/)
    // After:  expect(page).toHaveTitle(/Recently Viewed \\| Accounts/)
    
    // 6. Selector changed
    // Before: page.getByRole('button', { name: 'Submit' })
    // After:  page.getByRole('button', { name: 'Save' })
    
    // Dynamic text
    // Before: expect(page).toHaveText('Lead "John Doe" was created')
    // After:  expect(page).toHaveText(/Lead ".+" was created/)
    
    // Better assertion
    // Before: expect(element).toHaveText('exact text')
    // After:  expect(element).toContainText('partial text')
    
    // Test can't be fixed reliably
    test.fixme('scenario name', async ({ page }) => {
      // TODO: This test is flaky due to dynamic IDs in production
      // The search results contain session-specific IDs that change
      // Needs API mocking or test data reset capability
    });
    ```
    
    **IMPORTANT:**
    - NEVER use browser_snapshot, browser_debug, or any browser inspection tools
    - ALWAYS work from error messages only - they contain everything needed
    - ALWAYS try pattern matching from COMMON FIXES first
    - Maximum 2 fix attempts per test, then mark as fixme
    - After each test fix, summarize: "Fixed test X: Changed Y to Z"
    - Keep summaries brief to avoid context buildup
    
    **WHY NO BROWSER DEBUGGING:**
    Playwright error messages show:
    - Exact error type and location
    - All matching elements with their attributes
    - Suggested locators (e.g., "aka getByRole('textbox')")
    - Expected vs actual values
    This is sufficient to fix 99% of issues without browser inspection.

  expected_output: >
    Fixed test files with all tests passing. Output must include:
    - List of tests that failed initially
    - Root cause identified for each failure
    - Specific changes made to fix each test
    - Confirmation that fixed tests now pass
    - Any tests marked as test.fixme() with explanation
    - Summary report of healing results
